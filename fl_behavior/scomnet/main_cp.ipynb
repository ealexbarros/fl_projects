{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):       \n",
    "    list_set = set(list1) \n",
    "    unique_list = (list(list_set)) \n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "def create_userids( df ):\n",
    "    array = df.values\n",
    "    y = array[:, -1]\n",
    "    return unique( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "    RANDOM_STATE = 11235\n",
    "    \n",
    "    userids = create_userids(df)\n",
    "    nbclasses = len(userids)    \n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    mini_batch_size = int(min(X_train.shape[0]/10, 32))\n",
    "        \n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    X_val = np.asarray(X_val).astype(np.float32)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    print(mini_batch_size)\n",
    "    \n",
    "    BATCH_SIZE = mini_batch_size\n",
    "    SHUFFLE_BUFFER_SIZE = 100\n",
    "    \n",
    "    train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    return train_ds, val_ds, X_test, y_test, nbclasses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # config\n",
    "    BASE_PATH = \"/home/joaoneto/biometria/csv_files\"\n",
    "    MIN_NUM_ROWS = 50000\n",
    "    MAX_NUM_ROWS = 100000\n",
    "    NUM_FRAMES = 1000\n",
    "\n",
    "    users_statistics = pd.read_csv(\"users_statistics.csv\")\n",
    "    valid_users = users_statistics[(users_statistics.nrows >= MIN_NUM_ROWS) & (users_statistics.nrows <= MAX_NUM_ROWS)][\"player_id\"].unique()\n",
    "\n",
    "    tmp_data = []\n",
    "    for user in valid_users:\n",
    "        for csv_file_path in glob.glob(f\"{BASE_PATH}/{user}/*.csv\"):\n",
    "            tmp_data.append(pd.read_csv(csv_file_path))\n",
    "            \n",
    "    data = pd.concat(tmp_data)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    users = data['player_id'].unique()\n",
    "    \n",
    "    train_set, user_list = split_data(data, users, NUM_FRAMES)\n",
    "    train_set = np.array([np.array(x) for x in train_set]) \n",
    "    train_set_join = train_set.reshape(train_set.shape[0], 384)\n",
    "    \n",
    "    data_join = pd.DataFrame(train_set_join)\n",
    "    data_join['user'] = user_list\n",
    "    \n",
    "    train_ds, val_ds, X_test, y_test, n = split_dataframe(data_join)\n",
    "    \n",
    "    return train_ds, val_ds, X_test, y_test, n\n",
    "    \n",
    "def split_data(data, users, num_frames):\n",
    "    user_list = []\n",
    "    train = []\n",
    "    frame_size = 128\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        \n",
    "        for w in random.sample(range(0, data_user.shape[0] - frame_size, step), num_frames):\n",
    "            end = w + frame_size        \n",
    "            frame = data_user.iloc[w:end,[0, 1, 2]]        \n",
    "            train.append(frame)\n",
    "            user_list.append(user)\n",
    "\n",
    "    return train, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    train_dataset, validation_dataset, X_test, y_test, n = load_data()\n",
    "    return train_dataset, validation_dataset, X_test, y_test, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_training_loop(train_dataset, validation_dataset, X_test, y_test, nbclasses, input_shape = (128, 3), num_filters = 128):\n",
    "    input_layer = keras.layers.Input(input_shape) \n",
    "\n",
    "    conv1 = keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same')(input_layer)\n",
    "    conv1 = keras.layers.BatchNormalization()(conv1)\n",
    "    conv1 = keras.layers.Activation(activation='relu')(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same')(conv1)\n",
    "    conv2 = keras.layers.BatchNormalization()(conv2)\n",
    "    conv2 = keras.layers.Activation('relu')(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv1D(num_filters, kernel_size=3,padding='same')(conv2)\n",
    "    conv3 = keras.layers.BatchNormalization()(conv3)\n",
    "    conv3 = keras.layers.Activation('relu')(conv3)\n",
    "\n",
    "    gap_layer = keras.layers.GlobalAveragePooling1D()(conv3)\n",
    "    output_layer = keras.layers.Dense(nbclasses, activation='softmax')(gap_layer)\n",
    "    \n",
    "    model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    \n",
    "    learning_rate = 0.0001\n",
    "    cb = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=50, min_lr=learning_rate)\n",
    "    \n",
    "    precision = tf.keras.metrics.Precision(name='precision')\n",
    "    recall = tf.keras.metrics.Recall(name='recall')\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = keras.optimizers.Adam(), metrics=['categorical_accuracy', precision, recall]) \n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    EPOCHS = 500\n",
    "    \n",
    "    hist = model.fit(train_dataset, \n",
    "                  epochs=EPOCHS,\n",
    "                  verbose=True, \n",
    "                  validation_data=validation_dataset, \n",
    "                  callbacks=cb)\n",
    "    \n",
    "    hist_df = pd.DataFrame(hist.history) \n",
    "    type(hist_df)\n",
    "    print(\"HIST_DF CONTEUDO\")\n",
    "    print(hist_df)\n",
    "    hist_df.to_csv(\"1000_frames_1.csv\")\n",
    "    validation_metrics = model.evaluate(validation_dataset, return_dict=True)\n",
    "    print(\"Evaluating validation metrics\")\n",
    "    for m in model.metrics:\n",
    "        print(f\"\\t{m.name}: {validation_metrics[m.name]:.4f}\")        \n",
    "    \n",
    "    print(\"HEAD DO HIST_DF\")\n",
    "    hist_df.head(5)\n",
    "    # EVALUATION \n",
    "    X_test = np.asarray(X_test).astype(np.float32)    \n",
    "    y_true = np.argmax( y_test, axis=1)\n",
    "    y_pred = np.argmax( model.predict(X_test), axis=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)     \n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centralized_pipeline():\n",
    "    train_dataset, validation_dataset, X_test, y_test, n = get_datasets()\n",
    "    centralized_training_loop(train_dataset, validation_dataset, X_test, y_test, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab091ea862836714939f6a8a2c3cac24c2e0e5fe7c04b27710eeda60357feb6e"
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
