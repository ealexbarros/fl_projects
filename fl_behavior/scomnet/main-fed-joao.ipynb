{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "54bf7d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "import glob\n",
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fe749c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):       \n",
    "    list_set = set(list1) \n",
    "    unique_list = (list(list_set)) \n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "def create_userids( df ):\n",
    "    array = df.values\n",
    "    y = array[:, -1]\n",
    "    return unique( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a9375389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "    train_X = df.iloc[:, :384].values\n",
    "    le = LabelEncoder()\n",
    "    df['user'] = le.fit_transform(df['user'])\n",
    "    train_y = to_categorical(df['user']).astype(int)    \n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6629745",
   "metadata": {},
   "source": [
    "def split_dataframe(df):\n",
    "    RANDOM_STATE = 11235\n",
    "    \n",
    "    userids = create_userids(df)\n",
    "    nbclasses = len(userids)    \n",
    "    array = df.values\n",
    "    nsamples, nfeatures = array.shape\n",
    "    nfeatures = nfeatures -1 \n",
    "    X = array[:,0:nfeatures]\n",
    "    y = array[:,-1]\n",
    "    \n",
    "    enc = OneHotEncoder()\n",
    "    enc.fit(y.reshape(-1,1))\n",
    "    y = enc.transform(y.reshape(-1, 1)).toarray()\n",
    "    X = X.reshape(-1, 128, 3)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=RANDOM_STATE)\n",
    "    \n",
    "    mini_batch_size = int(min(X_train.shape[0]/10, 32))\n",
    "        \n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    X_val = np.asarray(X_val).astype(np.float32)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "\n",
    "    print(mini_batch_size)\n",
    "    \n",
    "    BATCH_SIZE = mini_batch_size\n",
    "    SHUFFLE_BUFFER_SIZE = 100\n",
    "    \n",
    "    train_ds = train_ds.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "    val_ds = val_ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    return train_ds, val_ds, X_test, y_test, nbclasses    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19910586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uniform_dataset(\n",
    "    X: np.ndarray, y: np.ndarray, number_of_clients: int\n",
    ") -> Tuple[Dict, tff.simulation.datasets.ClientData]:\n",
    "    \"\"\"Function distributes the data equally such that each client holds equal amounts of each class.\n",
    "    Args:\n",
    "        X (np.ndarray): Input.\\n\n",
    "        y (np.ndarray): Output.\\n\n",
    "        number_of_clients (int): Number of clients.\\n\n",
    "    Returns:\n",
    "        [Dict, tff.simulation.ClientData]: A dictionary and a tensorflow federated dataset containing the distributed dataset.\n",
    "    \"\"\"\n",
    "    clients_data = {f\"client_{i}\": [[], []] for i in range(1, number_of_clients + 1)}\n",
    "    for i in range(len(X)):\n",
    "        clients_data[f\"client_{(i%number_of_clients)+1}\"][0].append(X[i])\n",
    "        clients_data[f\"client_{(i%number_of_clients)+1}\"][1].append(y[i])\n",
    "\n",
    "    return clients_data, create_tff_dataset(clients_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "97144f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tff_dataset(clients_data: Dict) -> tff.simulation.datasets.ClientData:\n",
    "    \"\"\"Function converts dictionary to tensorflow federated dataset.\n",
    "    Args:\n",
    "        clients_data (Dict): Inputs.\n",
    "    Returns:\n",
    "        tff.simulation.ClientData: Returns federated data distribution.\n",
    "    \"\"\"\n",
    "    client_dataset = collections.OrderedDict()\n",
    "\n",
    "    for client in clients_data:\n",
    "        data = collections.OrderedDict(\n",
    "            (\n",
    "                (\"label\", np.array(clients_data[client][1], dtype=np.int32)),\n",
    "                (\"datapoints\", np.array(clients_data[client][0], dtype=np.float32)),\n",
    "            )\n",
    "        )\n",
    "        client_dataset[client] = data\n",
    "\n",
    "    return tff.simulation.datasets.TestClientData(client_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5239ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    epochs: int, batch_size: int, shuffle_buffer_size: int\n",
    ") -> Callable[[tf.data.Dataset], tf.data.Dataset]:\n",
    "    \"\"\"Function returns a function for preprocessing of a dataset.\n",
    "    Args:\n",
    "        epochs (int): How many times to repeat a batch.\\n\n",
    "        batch_size (int): Batch size.\\n\n",
    "        shuffle_buffer_size (int): Buffer size for shuffling the dataset.\\n\n",
    "    Returns:\n",
    "        Callable[[tf.data.Dataset], tf.data.Dataset]: A callable for preprocessing a dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def _reshape(element: collections.OrderedDict) -> tf.Tensor:\n",
    "\n",
    "        return (tf.expand_dims(element[\"datapoints\"], axis=-1), element[\"label\"])\n",
    "\n",
    "    @tff.tf_computation(\n",
    "        tff.SequenceType(\n",
    "            collections.OrderedDict(\n",
    "                label=tff.TensorType(tf.int32, shape=(30,)),\n",
    "                datapoints=tff.TensorType(tf.float32, shape=(384,)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    def preprocess(dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "        \"\"\"\n",
    "        Function returns shuffled dataset\n",
    "        \"\"\"\n",
    "        return (\n",
    "            dataset.shuffle(shuffle_buffer_size)\n",
    "            .repeat(epochs)\n",
    "            .batch(batch_size, drop_remainder=False)\n",
    "            .map(_reshape, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "02b0b40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" The code above does the following:\n",
    "    1. Loads the data from the csv files\n",
    "    2. Splits the data into train, validation and test sets\n",
    "    3. Returns the train, validation and test sets\n",
    "\"\"\"\n",
    "def load_data():\n",
    "    # config\n",
    "    BASE_PATH = \"/home/joaoneto/biometria/csv_files\"\n",
    "    MIN_NUM_ROWS = 50000\n",
    "    MAX_NUM_ROWS = 100000\n",
    "    NUM_FRAMES = 250\n",
    "    number_of_clients = 10\n",
    "\n",
    "    users_statistics = pd.read_csv(\"users_statistics.csv\")\n",
    "    valid_users = users_statistics[(users_statistics.nrows >= MIN_NUM_ROWS) & (users_statistics.nrows <= MAX_NUM_ROWS)][\"player_id\"].unique()\n",
    "\n",
    "    tmp_data = []\n",
    "    for user in valid_users:\n",
    "        for csv_file_path in glob.glob(f\"{BASE_PATH}/{user}/*.csv\"):\n",
    "            tmp_data.append(pd.read_csv(csv_file_path))\n",
    "            \n",
    "    data = pd.concat(tmp_data)\n",
    "    data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    users = data['player_id'].unique()\n",
    "    \n",
    "    train_set, user_list = split_data(data, users, NUM_FRAMES)\n",
    "    train_set = np.array([np.array(x) for x in train_set]) \n",
    "    train_set_join = train_set.reshape(train_set.shape[0], 384)\n",
    "    data_join = pd.DataFrame(train_set_join)\n",
    "    data_join['user'] = user_list\n",
    "    \n",
    "    train_X, train_y = split_dataframe(data_join)\n",
    "    train_client_data, train_data = create_uniform_dataset(train_X, train_y, number_of_clients)\n",
    "    \n",
    "    return train_data, len(train_X)\n",
    "    \n",
    "    \n",
    "def split_data(data, users, num_frames):\n",
    "    user_list = []\n",
    "    train = []\n",
    "    frame_size = 128\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        \n",
    "        for w in random.sample(range(0, data_user.shape[0] - frame_size, step), num_frames):\n",
    "            end = w + frame_size        \n",
    "            frame = data_user.iloc[w:end,[0, 1, 2]]        \n",
    "            train.append(frame)\n",
    "            user_list.append(user)\n",
    "\n",
    "    return train, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a467148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model_seq(input_shape, num_filters = 128, nbclasses = 30):\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(num_filters, kernel_size=3,padding='same', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(nbclasses, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a5fb2f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iterative_process_fn(\n",
    "    tff_model: tff.learning.Model,\n",
    "    server_optimizer_fn: Callable[[], tf.keras.optimizers.Optimizer],\n",
    "    client_optimizer_fn: Callable[[], tf.keras.optimizers.Optimizer] = None\n",
    ") -> tff.templates.IterativeProcess:\n",
    "    \"\"\"Function builds an iterative process that performs federated aggregation. The function offers federated averaging, federated stochastic gradient descent and robust federated aggregation.\n",
    "    Args:\n",
    "        tff_model (tff.learning.Model): Federated model object.\\n\n",
    "        server_optimizer_fn (Callable[[], tf.keras.optimizers.Optimizer]): Server optimizer function.\\n\n",
    "        aggregation_method (str, optional): Aggregation method. Defaults to \"fedavg\".\\n\n",
    "        client_optimizer_fn (Callable[[], tf.keras.optimizers.Optimizer], optional): Client optimizer function. Defaults to None.\\n\n",
    "        iterations (int, optional): [description]. Defaults to None.\\n\n",
    "        client_weighting (tff.learning.ClientWeighting, optional): Client weighting. Defaults to None.\\n\n",
    "        v (float, optional): L2 threshold. Defaults to None.\\n\n",
    "        compression (bool, optional): If the model should be compressed. Defaults to False.\\n\n",
    "        model_update_aggregation_factory (Callable[ [], tff.aggregators.UnweightedAggregationFactory ], optional): If the model should be trained with DP. Defaults to None.\\n\n",
    "    Returns:\n",
    "        tff.templates.IterativeProcess: An Iterative Process.\n",
    "    \"\"\"\n",
    "\n",
    "    return tff.learning.build_federated_averaging_process(\n",
    "                tff_model,\n",
    "                server_optimizer_fn = server_optimizer_fn,\n",
    "                client_optimizer_fn = client_optimizer_fn\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8f8316df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    \n",
    "    train_dataset, n = load_data()\n",
    "    \n",
    "    train_preprocess = preprocess_dataset(\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        shuffle_buffer_size=100,\n",
    "    )\n",
    "    \n",
    "    return train_dataset.preprocess(train_preprocess), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74a604d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IncompatiblePreprocessFnError",
     "evalue": "The preprocess_fn must not be a tff.Computation. Please use a python callable or tf.function instead. This restriction is because `tf.data.Dataset.map` wraps preprocessing functions with a `tf.function` decorator, which cannot call to a `tff.Computation`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompatiblePreprocessFnError\u001b[0m             Traceback (most recent call last)",
      "\u001b[1;32m/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m train_dataset, len_train_X \u001b[39m=\u001b[39m get_datasets()\n",
      "\u001b[1;32m/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb Cell 11'\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=2'>3</a>\u001b[0m train_dataset, n \u001b[39m=\u001b[39m load_data()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=4'>5</a>\u001b[0m train_preprocess \u001b[39m=\u001b[39m preprocess_dataset(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=5'>6</a>\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=6'>7</a>\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=7'>8</a>\u001b[0m     shuffle_buffer_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000009vscode-remote?line=10'>11</a>\u001b[0m \u001b[39mreturn\u001b[39;00m train_dataset\u001b[39m.\u001b[39;49mpreprocess(train_preprocess), n\n",
      "File \u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py:251\u001b[0m, in \u001b[0;36mClientData.preprocess\u001b[0;34m(self, preprocess_fn)\u001b[0m\n\u001b[1;32m    <a href='file:///home/joaoneto/anaconda3/envs/venv/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py?line=248'>249</a>\u001b[0m py_typecheck\u001b[39m.\u001b[39mcheck_callable(preprocess_fn)\n\u001b[1;32m    <a href='file:///home/joaoneto/anaconda3/envs/venv/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py?line=249'>250</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(preprocess_fn, computation_base\u001b[39m.\u001b[39mComputation):\n\u001b[0;32m--> <a href='file:///home/joaoneto/anaconda3/envs/venv/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py?line=250'>251</a>\u001b[0m   \u001b[39mraise\u001b[39;00m IncompatiblePreprocessFnError()\n\u001b[1;32m    <a href='file:///home/joaoneto/anaconda3/envs/venv/lib/python3.9/site-packages/tensorflow_federated/python/simulation/datasets/client_data.py?line=251'>252</a>\u001b[0m \u001b[39mreturn\u001b[39;00m PreprocessClientData(\u001b[39mself\u001b[39m, preprocess_fn)\n",
      "\u001b[0;31mIncompatiblePreprocessFnError\u001b[0m: The preprocess_fn must not be a tff.Computation. Please use a python callable or tf.function instead. This restriction is because `tf.data.Dataset.map` wraps preprocessing functions with a `tf.function` decorator, which cannot call to a `tff.Computation`."
     ]
    }
   ],
   "source": [
    "train_dataset, len_train_X = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5b12a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(optimizer_fn, optimizer_lr):\n",
    "    update\n",
    "    return _optimizer(learning_rate=learning_rate, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757e7c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef get_optimizer():\\n    return tf.keras.optimizers.SGD(learning_rate=0.01)\\n\\nserver_optimizer_fn = get_optimizer(server_optimizer_fn, server_optimizer_lr)\\nclient_optimizer_fn = get_optimizer(client_optimizer_fn, client_optimizer_lr)\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server_optimizer_fn = get_optimizer(server_optimizer_fn, server_optimizer_lr)\n",
    "client_optimizer_fn = get_optimizer(client_optimizer_fn, client_optimizer_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b235f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BatchDataset' object has no attribute 'create_tf_dataset_for_client'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000012vscode-remote?line=0'>1</a>\u001b[0m input_spec \u001b[39m=\u001b[39m train_dataset\u001b[39m.\u001b[39;49mcreate_tf_dataset_for_client(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000012vscode-remote?line=1'>2</a>\u001b[0m     train_dataset\u001b[39m.\u001b[39mclient_ids[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000012vscode-remote?line=2'>3</a>\u001b[0m )\u001b[39m.\u001b[39melement_spec\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000012vscode-remote?line=4'>5</a>\u001b[0m keras_model_fn \u001b[39m=\u001b[39m create_keras_model_seq((\u001b[39m128\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B192.168.1.2/home/joaoneto/biometria/fl_behavior/fl_projects/fl_behavior/scomnet/main-fed-joao.ipynb#ch0000012vscode-remote?line=5'>6</a>\u001b[0m get_keras_model \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(keras_model_fn)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BatchDataset' object has no attribute 'create_tf_dataset_for_client'"
     ]
    }
   ],
   "source": [
    "input_spec = train_dataset.create_tf_dataset_for_client(\n",
    "    train_dataset.client_ids[0]\n",
    ").element_spec\n",
    "\n",
    "keras_model_fn = create_keras_model_seq((128,3))\n",
    "get_keras_model = functools.partial(keras_model_fn)\n",
    "\n",
    "loss_fn = lambda: tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics_fn = lambda: [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "\n",
    "def model_fn() -> tff.learning.Model:\n",
    "    \"\"\"\n",
    "    Function that takes a keras model and creates an tensorflow federated learning model.\n",
    "    \"\"\"\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model=get_keras_model(),\n",
    "        input_spec=input_spec,\n",
    "        loss=loss_fn(),\n",
    "        metrics=metrics_fn(),\n",
    "    )\n",
    "\n",
    "iterative_process = iterative_process_fn(\n",
    "    model_fn,\n",
    "    server_optimizer_fn,\n",
    "    client_optimizer_fn=client_optimizer_fn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab091ea862836714939f6a8a2c3cac24c2e0e5fe7c04b27710eeda60357feb6e"
  },
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
