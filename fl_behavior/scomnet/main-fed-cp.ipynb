{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cf748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "\n",
    "import tensorflow_federated as tff\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6787f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(list1):       \n",
    "    list_set = set(list1) \n",
    "    unique_list = (list(list_set)) \n",
    "    unique_list.sort()\n",
    "    return unique_list\n",
    "\n",
    "def create_userids( df ):\n",
    "    array = df.values\n",
    "    y = array[:, -1]\n",
    "    return unique( y )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9375389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataframe(df):\n",
    "    train_X = df.iloc[:, :384].values\n",
    "    le = LabelEncoder()\n",
    "    df['user'] = le.fit_transform(df['user'])\n",
    "    train_y = to_categorical(df['user']).astype(int)    \n",
    "    return train_X, train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5b15a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-ae23f490a7f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m def create_uniform_dataset(\n\u001b[1;32m      2\u001b[0m     \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_clients\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m ) -> Tuple[Dict, tff.simulation.datasets.ClientData]:\n\u001b[0m\u001b[1;32m      4\u001b[0m     \"\"\"Function distributes the data equally such that each client holds equal amounts of each class.\n\u001b[1;32m      5\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "def create_uniform_dataset(\n",
    "    X: np.ndarray, y: np.ndarray, number_of_clients: int\n",
    ") -> Tuple[Dict, tff.simulation.datasets.ClientData]:\n",
    "    \"\"\"Function distributes the data equally such that each client holds equal amounts of each class.\n",
    "    Args:\n",
    "        X (np.ndarray): Input.\\n\n",
    "        y (np.ndarray): Output.\\n\n",
    "        number_of_clients (int): Number of clients.\\n\n",
    "    Returns:\n",
    "        [Dict, tff.simulation.ClientData]: A dictionary and a tensorflow federated dataset containing the distributed dataset.\n",
    "    \"\"\"\n",
    "    clients_data = {f\"client_{i}\": [[], []] for i in range(1, number_of_clients + 1)}\n",
    "    for i in range(len(X)):\n",
    "        clients_data[f\"client_{(i%number_of_clients)+1}\"][0].append(X[i])\n",
    "        clients_data[f\"client_{(i%number_of_clients)+1}\"][1].append(y[i])\n",
    "\n",
    "    return clients_data, create_tff_dataset(clients_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf2e490f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-fd210e75a5ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_tff_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclients_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClientData\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \"\"\"Function converts dictionary to tensorflow federated dataset.\n\u001b[1;32m      3\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mclients_data\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dict' is not defined"
     ]
    }
   ],
   "source": [
    "def create_tff_dataset(clients_data: Dict) -> tff.simulation.datasets.ClientData:\n",
    "    \"\"\"Function converts dictionary to tensorflow federated dataset.\n",
    "    Args:\n",
    "        clients_data (Dict): Inputs.\n",
    "    Returns:\n",
    "        tff.simulation.ClientData: Returns federated data distribution.\n",
    "    \"\"\"\n",
    "    client_dataset = collections.OrderedDict()\n",
    "\n",
    "    for client in clients_data:\n",
    "        data = collections.OrderedDict(\n",
    "            (\n",
    "                (\"label\", np.array(clients_data[client][1], dtype=np.int32)),\n",
    "                (\"datapoints\", np.array(clients_data[client][0], dtype=np.float32)),\n",
    "            )\n",
    "        )\n",
    "        client_dataset[client] = data\n",
    "\n",
    "    return tff.simulation.FromTensorSlicesClientData(client_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fff240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(\n",
    "    epochs: int, batch_size: int, shuffle_buffer_size: int\n",
    ") -> Callable[[tf.data.Dataset], tf.data.Dataset]:\n",
    "    \"\"\"Function returns a function for preprocessing of a dataset.\n",
    "    Args:\n",
    "        epochs (int): How many times to repeat a batch.\\n\n",
    "        batch_size (int): Batch size.\\n\n",
    "        shuffle_buffer_size (int): Buffer size for shuffling the dataset.\\n\n",
    "    Returns:\n",
    "        Callable[[tf.data.Dataset], tf.data.Dataset]: A callable for preprocessing a dataset object.\n",
    "    \"\"\"\n",
    "\n",
    "    def _reshape(element: collections.OrderedDict) -> tf.Tensor:\n",
    "\n",
    "        return (tf.expand_dims(element[\"datapoints\"], axis=-1), element[\"label\"])\n",
    "\n",
    "    @tff.tf_computation(\n",
    "        tff.SequenceType(\n",
    "            collections.OrderedDict(\n",
    "                label=tff.TensorType(tf.int32, shape=(30,)),\n",
    "                datapoints=tff.TensorType(tf.float32, shape=(384,)),\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    def preprocess(dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "        \"\"\"\n",
    "        Function returns shuffled dataset\n",
    "        \"\"\"\n",
    "        return (\n",
    "            dataset.shuffle(shuffle_buffer_size)\n",
    "            .repeat(epochs)\n",
    "            .batch(batch_size, drop_remainder=False)\n",
    "            .map(_reshape, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "    return preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68e1828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    screens = ['Focus', 'Mathisis', 'Memoria', 'Reacton', 'Speedy']\n",
    "    screens_code = ['1', '2', '3', '4', '5']\n",
    "    number_of_clients = 10\n",
    "\n",
    "    base_path = \"C:/Users/SouthSystem/Federated Learning/DataBioCom/data\"\n",
    "    phone_accel_file_paths = []\n",
    "\n",
    "    for directories, subdirectories, files in os.walk(base_path):\n",
    "        for filename in files:\n",
    "            if \"accel\" in filename:\n",
    "                phone_accel_file_paths.append(f\"{base_path}/accel/{filename}\")\n",
    "\n",
    "    data = pd.concat(map(pd.read_csv, phone_accel_file_paths))\n",
    "    users = data['player_id'].unique()\n",
    "    \n",
    "    train_set, user_list = split_data(data, users)\n",
    "    train_set = np.array([np.array(x) for x in train_set]) \n",
    "    train_set_join = train_set.reshape(train_set.shape[0], 384)\n",
    "    data_join = pd.DataFrame(train_set_join)\n",
    "    data_join['user'] = user_list\n",
    "    \n",
    "    train_X, train_y = split_dataframe(data_join)\n",
    "    train_client_data, train_data = create_uniform_dataset(train_X, train_y, number_of_clients)\n",
    "    \n",
    "    return train_data, len(train_X)\n",
    "\n",
    "def split_data(data, users):\n",
    "    user_list = []\n",
    "    train = []\n",
    "    frame_size = 128\n",
    "    step = 50\n",
    "\n",
    "    for user in users:\n",
    "        data_user = data[data['player_id']==user]  \n",
    "        data_user = data_user.iloc[:,[0,1,2]]\n",
    "        for w in range(0, data_user.shape[0] - frame_size, step):\n",
    "            end = w + frame_size        \n",
    "            frame = data_user.iloc[w:end,[0, 1, 2]]        \n",
    "            train.append(frame)\n",
    "            user_list.append(user)\n",
    "\n",
    "    return train, user_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbc53c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_keras_model_seq(input_shape, num_filters = 128, nbclasses = 30):\n",
    "    model = Sequential()\n",
    "    model.add(keras.layers.Conv1D(filters=num_filters, kernel_size=8, padding='same', activation='relu', input_shape=input_shape))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(filters=2*num_filters, kernel_size=5, padding='same', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Conv1D(num_filters, kernel_size=3,padding='same', activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.GlobalAveragePooling1D())\n",
    "    model.add(keras.layers.Dense(nbclasses, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1275d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iterative_process_fn(\n",
    "    tff_model: tff.learning.Model,\n",
    "    server_optimizer_fn: Callable[[], tf.keras.optimizers.Optimizer],\n",
    "    client_optimizer_fn: Callable[[], tf.keras.optimizers.Optimizer] = None\n",
    ") -> tff.templates.IterativeProcess:\n",
    "    \"\"\"Function builds an iterative process that performs federated aggregation. The function offers federated averaging, federated stochastic gradient descent and robust federated aggregation.\n",
    "    Args:\n",
    "        tff_model (tff.learning.Model): Federated model object.\\n\n",
    "        server_optimizer_fn (Callable[[], tf.keras.optimizers.Optimizer]): Server optimizer function.\\n\n",
    "        aggregation_method (str, optional): Aggregation method. Defaults to \"fedavg\".\\n\n",
    "        client_optimizer_fn (Callable[[], tf.keras.optimizers.Optimizer], optional): Client optimizer function. Defaults to None.\\n\n",
    "        iterations (int, optional): [description]. Defaults to None.\\n\n",
    "        client_weighting (tff.learning.ClientWeighting, optional): Client weighting. Defaults to None.\\n\n",
    "        v (float, optional): L2 threshold. Defaults to None.\\n\n",
    "        compression (bool, optional): If the model should be compressed. Defaults to False.\\n\n",
    "        model_update_aggregation_factory (Callable[ [], tff.aggregators.UnweightedAggregationFactory ], optional): If the model should be trained with DP. Defaults to None.\\n\n",
    "    Returns:\n",
    "        tff.templates.IterativeProcess: An Iterative Process.\n",
    "    \"\"\"\n",
    "\n",
    "    return tff.learning.build_federated_averaging_process(\n",
    "                tff_model,\n",
    "                server_optimizer_fn=server_optimizer_fn,\n",
    "                client_optimizer_fn=client_optimizer_fn\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2a4336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets():\n",
    "    \n",
    "    train_dataset, n = load_data()\n",
    "    \n",
    "    train_preprocess = preprocess_dataset(\n",
    "        epochs=10,\n",
    "        batch_size=32,\n",
    "        shuffle_buffer_size=100,\n",
    "    )\n",
    "    \n",
    "    return train_dataset.preprocess(train_preprocess), n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edbfa7f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30229/472676537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_train_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_30229/3652665249.py\u001b[0m in \u001b[0;36mget_datasets\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     train_preprocess = preprocess_dataset(\n",
      "\u001b[0;32m/tmp/ipykernel_30229/89975005.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mphone_accel_file_paths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{base_path}/accel/{filename}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphone_accel_file_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'player_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "train_dataset, len_train_X = get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "373fe92f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_optimizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-166dd490af41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mserver_optimizer_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_optimizer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_optimizer_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclient_optimizer_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_optimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_optimizer_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_optimizer_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_optimizer' is not defined"
     ]
    }
   ],
   "source": [
    "server_optimizer_fn = get_optimizer(server_optimizer_fn, server_optimizer_lr)\n",
    "client_optimizer_fn = get_optimizer(client_optimizer_fn, client_optimizer_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb87b534",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-9bb22e1d08f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m input_spec = train_dataset.create_tf_dataset_for_client(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m ).element_spec\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mkeras_model_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_keras_model_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "input_spec = train_dataset.create_tf_dataset_for_client(\n",
    "    train_dataset.client_ids[0]\n",
    ").element_spec\n",
    "\n",
    "keras_model_fn = create_keras_model_seq((128,3))\n",
    "get_keras_model = functools.partial(keras_model_fn)\n",
    "\n",
    "loss_fn = lambda: tf.keras.losses.CategoricalCrossentropy()\n",
    "metrics_fn = lambda: [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "\n",
    "def model_fn() -> tff.learning.Model:\n",
    "    \"\"\"\n",
    "    Function that takes a keras model and creates an tensorflow federated learning model.\n",
    "    \"\"\"\n",
    "    return tff.learning.from_keras_model(\n",
    "        keras_model=get_keras_model(),\n",
    "        input_spec=input_spec,\n",
    "        loss=loss_fn(),\n",
    "        metrics=metrics_fn(),\n",
    "    )\n",
    "\n",
    "iterative_process = iterative_process_fn(\n",
    "    model_fn,\n",
    "    server_optimizer_fn,\n",
    "    client_optimizer_fn=client_optimizer_fn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ab091ea862836714939f6a8a2c3cac24c2e0e5fe7c04b27710eeda60357feb6e"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
